from google import genai
import requests
import time
import json
import config

# Configuration
API_URL = "http://localhost:5000/api"
MODEL_NAME = "gemma-3-12b-it" 

# Setup Gemini Client (New SDK 1.0 architecture)
client = genai.Client(api_key=config.GEMINI_API_KEY)

def get_simulation_state(retries=3):
    for i in range(retries):
        try:
            response = requests.get(f"{API_URL}/state", timeout=5)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            if i < retries - 1:
                print(f"‚ö†Ô∏è State API temporary unavailable, retrying ({i+1}/{retries})...")
                time.sleep(2)
            else:
                print(f"‚ùå Error fetching state after {retries} attempts: {e}")
    return None

def get_grid_heatmap(retries=2):
    for i in range(retries):
        try:
            response = requests.get(f"{API_URL}/grid/heatmap", timeout=5)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            if i < retries - 1:
                time.sleep(1)
            else:
                print(f"Error fetching heatmap: {e}")
    return None

def decide_action(state, heatmap):
    # Construct prompt
    # Extract detailed metrics
    agents_details = state.get("agents_details", [])
    agents_alive = len(agents_details)
    agents_dead = state.get("agents_dead", 0)
    step = state.get("step", 0)
    
    # Calculate stats manually since API no longer gives averages
    avg_energy = sum(a['energy'] for a in agents_details) / max(1, agents_alive)
    avg_temp = sum(a['temp'] for a in agents_details) / max(1, agents_alive)
    avg_valence = sum(a['valence'] for a in agents_details) / max(1, agents_alive)
    
    # Find critical agents (Low Energy AND/OR Low Valence)
    # Critical Energy < 50.0, Low Valence < -0.5
    critical_agents = [
        a for a in agents_details 
        if a['energy'] < 50.0 or a['valence'] < -0.5
    ]
    
    # Format critical agents for the Prompt
    critical_info = ""
    if critical_agents:
        critical_info = "CRITICAL ALERTS:\n"
        for a in critical_agents[:10]: # Limit to top 10 to save tokens
            critical_info += f"- Agent {a['id']} at ({a['x']}, {a['y']}): Energy={a['energy']}, Valence={a['valence']}\n"
    else:
        critical_info = "Status OK: No agents in critical condition."

    heatmap_summary = json.dumps(heatmap.get("heatmap", [])[:20]) 
    
    prompt = f"""
    You are the benevolent Overseer of a digital ant farm simulation.
    
    Current Status:
    - Step: {step}
    - Agents Alive: {agents_alive}
    - Agents Dead: {agents_dead}
    - Global Avg Energy: {avg_energy:.2f}
    - Global Avg Mood (Valence): {avg_valence:.2f}
    
    {critical_info}
    
    Significant Active Areas (x, y, intensity): {heatmap_summary}
    
    Your Goal:
    Observe the colony. PRIORITIZE keeping agents alive. 
    Look at the CRITICAL ALERTS. If an agent has low energy (<50) or negative valence, they are suffering.
    Drop food NEAR them (their x, y coordinates) to help.
    
    Response Format (JSON only):
    {{
        "thought": "Your reasoning here...",
        "action": "drop_food" or "wait",
        "x": <integer 0-79> (only if drop_food),
        "y": <integer 0-39> (only if drop_food),
        "amount": <float> (only if drop_food, default 30.0)
    }}
    """
    
    try:
        # Request JSON mode, but keep stripping logic for models that might ignore it
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )
        
        text = response.text.strip()
        # Fallback stripping for potential Markdown blocks from older models/Gemma
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]

        return json.loads(text.strip())
    except Exception as e:
        print(f"Error generating decision: {e}")
        return None

def execute_action(decision):
    if not decision: return
    
    action = decision.get("action")
    print(f"ü§î Thought: {decision.get('thought')}")
    
    if action == "drop_food":
        payload = {
            "x": decision.get("x", 40),
            "y": decision.get("y", 20),
            "amount": decision.get("amount", 30.0)
        }
        try:
            res = requests.post(f"{API_URL}/action/drop_food", json=payload)
            print(f"‚úÖ Action Executed: Dropped food at ({payload['x']}, {payload['y']})")
        except Exception as e:
            print(f"‚ùå Action Failed: {e}")
    else:
        print("zzz... Waiting.")

def main():
    print("ü§ñ LLM Agent Initialized. Connecting to Simulation...")
    
    # Wait for simulation to start
    time.sleep(2)
    
    while True:
        state = get_simulation_state()
        if state:
            heatmap = get_grid_heatmap()
            # Gemma 3 has 30 RPM, so we can poll more frequently (every 2-3 seconds)
            step = state.get("step", 0)
            
            # Decide every step or every few steps
            print(f"Step {step}: Thinking...")
            decision = decide_action(state, heatmap)
            if decision:
                execute_action(decision)
            
            # 30 RPM = 1 request every 2 seconds.
            time.sleep(2.5) 
        else:
            time.sleep(1.0) 

if __name__ == "__main__":
    main()
